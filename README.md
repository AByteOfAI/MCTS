# Monte Carlo Tree Search with Deep Reinforcement Learning for Robotic Navigation

<p align="center">
  <strong>EEE598 Final Project â€” Fall 2024 | Arizona State University</strong><br>
  Team 14: Sakshi Lathi Â· Abhijit Sinha Â· Anusha Chatterjee
</p>

---

## Overview

This repository contains our research paper and presentation for EEE598 (Fall 2024) at Arizona State University. We investigate **Monte Carlo Tree Search (MCTS)** and its integration with **Deep Reinforcement Learning (DRL)** for solving the **robotic follow-ahead navigation** problem â€” where a robot must lead a human target while dynamically avoiding obstacles and maintaining line-of-sight.

> **ğŸ“„ Full Paper:** [`Team14_Lathi_Sinha_Chatterjee_MonteCarlo.pdf`](Team14_Lathi_Sinha_Chatterjee_MonteCarlo.pdf)

---

## Problem Statement

In robotic follow-ahead scenarios, a robot must navigate *in front of* a human while simultaneously:

- **Predicting human intentions** â€” anticipating the target's future movement in real time
- **Avoiding obstacles** â€” dynamically replanning around physical barriers
- **Preventing occlusion** â€” maintaining an unobstructed line-of-sight to the human
- **Ensuring safety** â€” preventing collisions in shared human-robot spaces

Existing methods typically address follow-behind or side-by-side configurations and struggle with the vast state spaces and unpredictability inherent in leading a human through dynamic environments.

---

## How MCTS Works

MCTS builds a decision tree incrementally through four iterative stages:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Selection   â”‚  Navigate tree using UCB to balance exploration vs. exploitation
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Expansion   â”‚  Add child nodes representing unexplored actions
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Simulation  â”‚  Run rollouts to estimate future rewards
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Backpropagation  â”‚  Update node statistics along the selected path
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Node selection is guided by the **Upper Confidence Bound (UCB)** formula:

```
UCB = w / ná¶œ  +  c Â· âˆš(ln náµ– / ná¶œ)
```

| Symbol | Meaning |
|--------|---------|
| `w` | Value of the node (expected reward) |
| `ná¶œ` | Visit count of the child node |
| `náµ–` | Visit count of the parent node |
| `c` | Exploration constant (typically **1.4**) |

---

## MCTS-DRL Framework

The key contribution of our study is the integration of MCTS with a **trained DRL policy** that replaces random rollouts during the simulation phase:

```
  Human Trajectory        Occupancy Map        Robot Pose
  Prediction                   â”‚                   â”‚
       â”‚                      â”‚                   â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚   MCTS-DRL Engine   â”‚
       â”‚                     â”‚
       â”‚  1. Expand tree     â”‚
       â”‚  2. Check collision â”‚â”€â”€â”€â”€ collision? â†’ prune node
       â”‚  3. Check occlusion â”‚â”€â”€â”€â”€ occluded? â†’ penalize (âˆ’1)
       â”‚  4. DRL evaluation  â”‚â”€â”€â”€â”€ Q(o, aáµ¢) reward estimate
       â”‚  5. Backpropagate   â”‚
       â”‚  6. Select best UCB â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
          Navigational Goal
           (c = 0, exploit)
```

---

## Key Results

### MCTS-DRL vs. Standalone Methods

| Metric | DRL Only | MCTS Only | **MCTS-DRL** |
|---|---|---|---|
| Trajectory Accuracy | Moderate | Inconsistent | **Excellent** |
| Obstacle Avoidance | Limited | Moderate | **High** |
| Occlusion Handling | Poor | Moderate | **High** |
| Mean Reward | âˆ’18.4 | 3.2 Â± 5.9 | **5.4** |

### Cumulative Rewards by Trajectory (20 trials)

| Human Trajectory | DRL | MCTS | **MCTS-DRL** |
|---|---|---|---|
| Circular | âˆ’17.95 | 2.87 Â± 5.96 | **4.53** |
| S-shaped | âˆ’21.84 | âˆ’3.83 Â± 4.33 | **âˆ’1.61** |

### SL-MCTS vs. Traditional MCTS

| Metric | Traditional MCTS | **SL-MCTS** |
|---|---|---|
| Success Rate | 78% | **92%** |
| Avg. Path Length | 15 steps | **12 steps** |
| Computation Time | 2.4s | **1.3s** |

The MCTS-DRL hybrid consistently outperforms both standalone approaches across straight, U-shaped, S-shaped, and L-shaped test trajectories.

---

## Applications Discussed

| Domain | Description |
|---|---|
| **Robotic Follow-Ahead** | Robot leads a human through dynamic environments with obstacle and occlusion avoidance |
| **Multi-Agent Pathfinding** | Autonomous warehouse robots navigating around shelves and each other |
| **Wearable Exoskeletons** | Real-time gait assistance that adapts to patient feedback |
| **Humanoid Robotics** | Task planning and safe human interaction (e.g., Tesla Optimus) |

---

## Repository Contents

```
MCTS/
â”œâ”€â”€ README.md                                        # This file
â””â”€â”€ Team14_Lathi_Sinha_Chatterjee_MonteCarlo.pdf     # Full paper + presentation slides
```

---

## References

1. Leisiazar, S., Park, E. J., Lim, A., & Chen, M. (2023). *An MCTS-DRL Based Obstacle and Occlusion Avoidance Methodology in Robotic Follow-Ahead Applications.*

2. Li, W., Liu, Y., Ma, Y., Xu, K., Qiu, J., & Gan, Z. (2023). *A Self-Learning Monte Carlo Tree Search Algorithm for Robot Path Planning.* Frontiers in Neurorobotics.

3. *Robust walking control of a lower limb rehabilitation exoskeleton coupled with a musculoskeletal model via deep reinforcement learning.*

---

## Team

| Name | Email |
|---|---|
| Sakshi Lathi | [slathi@asu.edu](mailto:slathi@asu.edu) |
| Abhijit Sinha | [asinh117@asu.edu](mailto:asinh117@asu.edu) |
| Anusha Chatterjee | [achatt53@asu.edu](mailto:achatt53@asu.edu) |

**Course:** EEE598 â€” Fall 2024, School of Electrical, Computer, and Energy Engineering, Arizona State University

---

<p align="center">
  <em>Built with â¤ï¸ at Arizona State University</em>
</p>
